{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0cab16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dacite\n",
    "\n",
    "from pipeline import Pipeline\n",
    "from pipeline_config import PipelineConfig\n",
    "from model_training.config import TrainConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e95ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaner = [\"UsernameRemover\"]\n",
    "\n",
    "pipeline_config = {\n",
    "                    \"text_cleaners\": [\"UsernameRemover\"]\n",
    "                    # List of text cleaning classes - see pipeline_steps/text_cleaning.py\n",
    "                   , \"max_seq_len\": 64}\n",
    "pipeline_config = dacite.from_dict(PipelineConfig, pipeline_config)\n",
    "# There is no much to configure in pipelines architecture\n",
    "#- BERT configuration for Polbert is already pre-defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e165520",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "                \"batch_size\": 32,\n",
    "                \"num_epochs\": 10,\n",
    "                \"output_model_name\": \"bert_for_hatespeech\",\n",
    "                # Model would be saved in the repo_root/trained_models/output_model_name\n",
    "                \"main_metric\": \"f1-score\",\n",
    "                # metric on which saving the best model would be done\n",
    "                \"freeze_embeddings\": True,\n",
    "                # whether to freeze embedding module during the training - size of the data is small\n",
    "                # and training with this module unfrozen is very prone to overfitting\n",
    "                \"class_balanced_sampling\": True,\n",
    "                # classes cyberbulling and hate-speech have small number of samples in the data\n",
    "                # thanks to using this sampler convergence of training is faster\n",
    "                \"optimization_schedule\": {\n",
    "                                            \"init_lr\": 1e-04,\n",
    "                                            # Initial value of learning rate\n",
    "                                            \"weight_decay\": 1e-03,\n",
    "                                            # Value of L2 regularization coefficient\n",
    "                                            \"num_warmup_steps\": 100,\n",
    "                                            # Number of warmup steps ( where learning rate increases from 0 to init_lr value)\n",
    "                                            \"optimizer_name\": \"adamw\"\n",
    "                                            # Name of optimizer to be used (adam, adamw and sgd are supported)\n",
    "                                         }\n",
    "                \n",
    "}\n",
    "train_config = dacite.from_dict(TrainConfig, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6ace0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f4630",
   "metadata": {},
   "source": [
    "The training data from http://2019.poleval.pl/index.php/tasks/task6 has been splited in proportion 0.8 for train, 0.2 for validation.\n",
    "\n",
    "It has been assumed that there are no leaks in data and the split could be done in pure random way.\n",
    "\n",
    "Even if there were some leaks no metadata was provided for the utterances so it would be hard. \n",
    "\n",
    "The test data is the official test data for the task\n",
    "\n",
    "The data (all splits) is kept within the repository in datafiles directory. Each split has two files: {split_name}_texts.txt and {split_name}_tags.txt first one corresponds to training utterances, second one to the labels.\n",
    "\n",
    "After each epoch the validation on valid dataset is run - when macro avg f1 score is higher than previous best ( previous best is initialized from 0. ) than the model is saved and we also run it over test and training set.\n",
    "\n",
    "For each split both per class precision, recall and f1 are calculated and micro and macro averaging of f1 ( as in evaluation http://2019.poleval.pl/index.php/results/ ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca16ce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-cased-v1 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler class weights = {0: 1.0, 2: 1.4443161466911736, 1: 1.686544469631841}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 epoch training in progress: 251it [01:21,  3.07it/s]\n",
      "Model evaluation on valid: 100%|██████████| 64/64 [00:06<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr valid\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      1846\n",
      "           1       0.24      0.16      0.19        58\n",
      "           2       0.65      0.27      0.39       124\n",
      "\n",
      "    accuracy                           0.92      2028\n",
      "   macro avg       0.61      0.47      0.51      2028\n",
      "weighted avg       0.90      0.92      0.90      2028\n",
      "\n",
      " micro f1: 0.9176528599605522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model evaluation on test: 100%|██████████| 32/32 [00:03<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr test\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94       866\n",
      "           1       0.22      0.16      0.19        25\n",
      "           2       0.74      0.13      0.22       109\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.62      0.43      0.45      1000\n",
      "weighted avg       0.86      0.88      0.84      1000\n",
      "\n",
      " micro f1: 0.878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model evaluation on train: 100%|██████████| 251/251 [00:26<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr train\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      7043\n",
      "           1       0.59      0.49      0.53       317\n",
      "           2       0.83      0.39      0.53       653\n",
      "\n",
      "    accuracy                           0.92      8013\n",
      "   macro avg       0.79      0.62      0.68      8013\n",
      "weighted avg       0.92      0.92      0.91      8013\n",
      "\n",
      " micro f1: 0.9239985024335455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epoch training in progress: 251it [01:23,  3.00it/s]\n",
      "Model evaluation on valid: 100%|██████████| 64/64 [00:06<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr valid\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1846\n",
      "           1       0.40      0.17      0.24        58\n",
      "           2       0.51      0.44      0.48       124\n",
      "\n",
      "    accuracy                           0.92      2028\n",
      "   macro avg       0.62      0.53      0.56      2028\n",
      "weighted avg       0.91      0.92      0.91      2028\n",
      "\n",
      " micro f1: 0.9181459566074951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model evaluation on test: 100%|██████████| 32/32 [00:03<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr test\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94       866\n",
      "           1       0.19      0.12      0.15        25\n",
      "           2       0.55      0.26      0.35       109\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.55      0.45      0.48      1000\n",
      "weighted avg       0.85      0.88      0.86      1000\n",
      "\n",
      " micro f1: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model evaluation on train: 100%|██████████| 251/251 [00:27<00:00,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr train\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      7014\n",
      "           1       0.88      0.81      0.84       327\n",
      "           2       0.88      0.82      0.85       672\n",
      "\n",
      "    accuracy                           0.97      8013\n",
      "   macro avg       0.91      0.88      0.89      8013\n",
      "weighted avg       0.97      0.97      0.97      8013\n",
      "\n",
      " micro f1: 0.9696742792961438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2 epoch training in progress: 251it [01:25,  2.94it/s]\n",
      "Model evaluation on valid: 100%|██████████| 64/64 [00:07<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr valid\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1846\n",
      "           1       0.24      0.34      0.28        58\n",
      "           2       0.76      0.36      0.49       124\n",
      "\n",
      "    accuracy                           0.92      2028\n",
      "   macro avg       0.65      0.56      0.58      2028\n",
      "weighted avg       0.92      0.92      0.91      2028\n",
      "\n",
      " micro f1: 0.9166666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model evaluation on test: 100%|██████████| 32/32 [00:03<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr test\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94       866\n",
      "           1       0.18      0.28      0.22        25\n",
      "           2       0.67      0.09      0.16       109\n",
      "\n",
      "    accuracy                           0.87      1000\n",
      "   macro avg       0.58      0.45      0.44      1000\n",
      "weighted avg       0.86      0.87      0.84      1000\n",
      "\n",
      " micro f1: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model evaluation on train: 100%|██████████| 251/251 [00:27<00:00,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr train\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      7097\n",
      "           1       0.67      0.94      0.78       297\n",
      "           2       0.98      0.74      0.84       619\n",
      "\n",
      "    accuracy                           0.97      8013\n",
      "   macro avg       0.88      0.89      0.87      8013\n",
      "weighted avg       0.98      0.97      0.97      8013\n",
      "\n",
      " micro f1: 0.9714214401597404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3 epoch training in progress: 251it [01:25,  2.92it/s]\n",
      "Model evaluation on valid: 100%|██████████| 64/64 [00:07<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr valid\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96      1846\n",
      "           1       0.35      0.26      0.30        58\n",
      "           2       0.64      0.43      0.51       124\n",
      "\n",
      "    accuracy                           0.93      2028\n",
      "   macro avg       0.65      0.56      0.59      2028\n",
      "weighted avg       0.91      0.93      0.92      2028\n",
      "\n",
      " micro f1: 0.9250493096646942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model evaluation on test: 100%|██████████| 32/32 [00:03<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr test\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       866\n",
      "           1       0.17      0.16      0.17        25\n",
      "           2       0.63      0.16      0.25       109\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.57      0.44      0.45      1000\n",
      "weighted avg       0.86      0.88      0.85      1000\n",
      "\n",
      " micro f1: 0.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model evaluation on train: 100%|██████████| 251/251 [00:27<00:00,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr train\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7025\n",
      "           1       0.77      0.96      0.85       310\n",
      "           2       0.98      0.85      0.91       678\n",
      "\n",
      "    accuracy                           0.98      8013\n",
      "   macro avg       0.91      0.93      0.92      8013\n",
      "weighted avg       0.98      0.98      0.98      8013\n",
      "\n",
      " micro f1: 0.9812804193186072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4 epoch training in progress: 251it [01:25,  2.92it/s]\n",
      "Model evaluation on valid: 100%|██████████| 64/64 [00:07<00:00,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr valid\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      1846\n",
      "           1       0.39      0.12      0.18        58\n",
      "           2       0.64      0.39      0.48       124\n",
      "\n",
      "    accuracy                           0.93      2028\n",
      "   macro avg       0.66      0.50      0.54      2028\n",
      "weighted avg       0.91      0.93      0.91      2028\n",
      "\n",
      " micro f1: 0.925542406311637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epoch training in progress: 251it [01:25,  2.92it/s]\n",
      "Model evaluation on valid: 100%|██████████| 64/64 [00:07<00:00,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr valid\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1846\n",
      "           1       0.45      0.16      0.23        58\n",
      "           2       0.68      0.44      0.54       124\n",
      "\n",
      "    accuracy                           0.93      2028\n",
      "   macro avg       0.69      0.53      0.58      2028\n",
      "weighted avg       0.92      0.93      0.92      2028\n",
      "\n",
      " micro f1: 0.9309664694280079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6 epoch training in progress: 251it [01:25,  2.92it/s]\n",
      "Model evaluation on valid: 100%|██████████| 64/64 [00:07<00:00,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr valid\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      1846\n",
      "           1       0.47      0.16      0.23        58\n",
      "           2       0.45      0.58      0.51       124\n",
      "\n",
      "    accuracy                           0.91      2028\n",
      "   macro avg       0.63      0.56      0.57      2028\n",
      "weighted avg       0.91      0.91      0.91      2028\n",
      "\n",
      " micro f1: 0.9097633136094675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7 epoch training in progress: 251it [01:25,  2.92it/s]\n",
      "Model evaluation on valid: 100%|██████████| 64/64 [00:07<00:00,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr valid\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      1846\n",
      "           1       0.67      0.14      0.23        58\n",
      "           2       0.66      0.45      0.54       124\n",
      "\n",
      "    accuracy                           0.93      2028\n",
      "   macro avg       0.76      0.53      0.58      2028\n",
      "weighted avg       0.92      0.93      0.92      2028\n",
      "\n",
      " micro f1: 0.9304733727810651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8 epoch training in progress: 251it [01:25,  2.92it/s]\n",
      "Model evaluation on valid: 100%|██████████| 64/64 [00:07<00:00,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr valid\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      1846\n",
      "           1       0.38      0.14      0.20        58\n",
      "           2       0.65      0.44      0.53       124\n",
      "\n",
      "    accuracy                           0.93      2028\n",
      "   macro avg       0.66      0.52      0.56      2028\n",
      "weighted avg       0.91      0.93      0.92      2028\n",
      "\n",
      " micro f1: 0.9265285996055227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9 epoch training in progress: 251it [01:25,  2.92it/s]\n",
      "Model evaluation on valid: 100%|██████████| 64/64 [00:07<00:00,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results fpr valid\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      1846\n",
      "           1       0.35      0.12      0.18        58\n",
      "           2       0.70      0.44      0.54       124\n",
      "\n",
      "    accuracy                           0.93      2028\n",
      "   macro avg       0.66      0.51      0.56      2028\n",
      "weighted avg       0.91      0.93      0.92      2028\n",
      "\n",
      " micro f1: 0.9285009861932939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(pipeline_config=pipeline_config)\n",
    "pipeline.train(train_config=train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f86670",
   "metadata": {},
   "source": [
    "As you may see we are achiving micro avg f1 close to 0.90 and macro avg f1 ~0.5 which is close to the best models trained during poleval competition - for need of this homework we find it sufficient, however no grid search over training hyperparameters (like dropout, weight decay etc.) has been done.\n",
    "\n",
    "Probably pre-training BERT language model on large set of social media data could significantly improve the results. Also provided data is just a flat list while Twitter discussion probably has some tree structure - providing somehow context of discussion to the model could give another significant improvement. \n",
    "\n",
    "We see that after a few epochs the model overfits highly to the train set which is not suprising.\n",
    "\n",
    "NOTE: after the training upload the weights to hatespeechml S3 bucket, for access contact kdziedzic66@gmail.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2d352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
